# -*- coding: utf-8 -*-
"""cerealpro

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dHfWkIZW4j3J43SmEQUlX2z5gRHhKE1a

Import cereal data uncleaned
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install -q tensorflow-model-optimization
!pip install ipython-autotime

# %load_ext autotime

import matplotlib.pyplot as plt
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
#import seaborn as sns
import tempfile
import os
import tensorflow as tf
import numpy as np
from tensorflow import keras
import matplotlib.pyplot as plt
# %load_ext tensorboard
# %matplotlib inline

from sklearn.model_selection import train_test_split

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

from google.colab import files
uploaded = files.upload()

cereal = pd.read_csv('cereal.csv')
cereal.head(10)

"""dropping useless info"""

cereal.drop(['name', 'type', 'weight', 'rating'], axis=1, inplace=True)
cereal_num = {'A': 0,'G': 1,'K': 2,'N': 3,'P': 4,'Q': 5,'R': 6}
cereal['mfr'] = cereal['mfr'].map(cereal_num)
cereal.head(10)

cereal.to_csv('cereal_cleaned.csv', index = False)

"""Cleaned up data"""

import pandas as pd
from sklearn.model_selection import train_test_split

cereal = pd.read_csv('cereal_cleaned.csv')
cereal.head(10)

"""Breaks up dataset into test, validation and training"""

features = cereal.drop('calories', axis=1)
labels = cereal['calories']

x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.4, random_state=42) #4 datasets created, 60 train
x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=42) #breaking up x,y test into test20 validation20

"""shows training 60%, validation 21%, test 19% close enough too 60/20/20 split"""

for dataset in [y_train, y_val, y_test]:
  print(round(len(dataset)/len(labels), 2))

"""Breaking up datasets for features and labels"""

x_train.to_csv('train_features.csv', index = False)
x_val.to_csv('val_features.csv', index = False)
x_test.to_csv('test_features.csv', index = False)

y_train.to_csv('train_labels.csv', index = False)
y_val.to_csv('val_labels.csv', index = False)
y_test.to_csv('test_labels.csv', index = False)

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score

tr_features = pd.read_csv('train_features.csv')
tr_labels = pd.read_csv('train_labels.csv')

#tr_features.head()
#tr_labels.head()
rf = RandomForestClassifier()

scores = cross_val_score(rf, tr_features, tr_labels.values.ravel(), cv=5)

scores

def print_results(results):
   print('Best Parameters: {}\n'.format(results.best_params_))

   means = results.cv_results_['mean_test_score']
   stds = results.cv_results_['std_test_score']
   for mean, std, params in zip(means, stds, results.cv_results_['params']):
     print('{} (+/-{}) for {}'.format(round(mean, 3), round(std*2, 3), params))

rf = RandomForestClassifier()
parameters = {
    'n_estimators' : [50, 100, 200, 400, 800],
    'max_depth' : [2, 10, 20, 40, 80]
}
#n_estimators is the number of trees built before taking the avg
#max_depth depth of tree
cv = GridSearchCV(rf, parameters, cv=5)
cv.fit(tr_features, tr_labels.values.ravel())

print_results(cv)

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score

tr_features = pd.read_csv('train_features.csv')
tr_labels = pd.read_csv('train_labels.csv')

val_features = pd.read_csv('val_features.csv')
val_labels = pd.read_csv('val_labels.csv')

te_features = pd.read_csv('test_features.csv')
te_labels = pd.read_csv('test_labels.csv')

rf1 = RandomForestClassifier(n_estimators=50, max_depth=20)
rf1.fit(tr_features, tr_labels.values.ravel())

rf2 = RandomForestClassifier(n_estimators=50, max_depth=40)
rf2.fit(tr_features, tr_labels.values.ravel())

rf3 = RandomForestClassifier(n_estimators=50, max_depth=80)
rf3.fit(tr_features, tr_labels.values.ravel())

for mdl in [rf1, rf2, rf3]:
  y_pred = mdl.predict(val_features)
  accuracy = round(accuracy_score(val_labels, y_pred),3)
  precision = round(precision_score(val_labels, y_pred, average='micro'),3)
  recall= round(recall_score(val_labels, y_pred, average='micro'), 3)
  print('Max Depth: {} / # Of Est: {} -- A: {}/ P: {}/ R: {}'.format(mdl.max_depth,mdl.n_estimators,accuracy,precision,recall))

X = cereal.drop(columns=['vitamins'])
y = cereal['vitamins']
print(X.shape)
print(y.shape)

x_train,x_test,y_train,y_test = train_test_split(X,y)
print(x_train.shape)
print(x_test.shape)

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn import tree

params = {'max_depth': [2,4,6,8,10,12],
         'min_samples_split': [2,3,4],
         'min_samples_leaf': [1,2]}

clf = tree.DecisionTreeClassifier(random_state=0)
gcv = GridSearchCV(estimator=clf,param_grid=params)
gcv.fit(x_train,y_train)

model = gcv.best_estimator_
model.fit(x_train,y_train)
y_train_pred = model.predict(x_train)
y_test_pred = model.predict(x_test)

print(f'Train score {accuracy_score(y_train_pred,y_train)}')
print(f'Test score {accuracy_score(y_test_pred,y_test)}')

path = clf.cost_complexity_pruning_path(x_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities
print(ccp_alphas)

clfs = []
for ccp_alpha in ccp_alphas:
    clf = tree.DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)
    clf.fit(x_train, y_train)
    clfs.append(clf)

clf = tree.DecisionTreeClassifier(random_state=0,ccp_alpha=0.020)
  clf.fit(x_train,y_train)
  y_train_pred = clf_.predict(x_train)
  y_test_pred = clf_.predict(x_test)

print(f'Train score {accuracy_score(y_train_pred,y_train)}')
print(f'Test score {accuracy_score(y_test_pred,y_test)}')

fig, ax = plt.subplots()
ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle="steps-post")
ax.set_xlabel("effective alpha")
ax.set_ylabel("total impurity of leaves")
ax.set_title("Total Impurity vs effective alpha for training set")

clfs = []
for ccp_alpha in ccp_alphas:
    clf = tree.DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)
    clf.fit(x_train, y_train)
    clfs.append(clf)
print("Number of nodes in the last tree is: {} with ccp_alpha: {}".format(
      clfs[-1].tree_.node_count, ccp_alphas[-1]))

clfs = clfs[:-1]
ccp_alphas = ccp_alphas[:-1]

node_counts = [clf.tree_.node_count for clf in clfs]
depth = [clf.tree_.max_depth for clf in clfs]
fig, ax = plt.subplots(2, 1)
ax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle="steps-post")
ax[0].set_xlabel("alpha")
ax[0].set_ylabel("number of nodes")
ax[0].set_title("Number of nodes vs alpha")
ax[1].plot(ccp_alphas, depth, marker='o', drawstyle="steps-post")
ax[1].set_xlabel("alpha")
ax[1].set_ylabel("depth of tree")
ax[1].set_title("Depth vs alpha")
fig.tight_layout()

train_scores = [clf.score(x_train, y_train) for clf in clfs]
test_scores = [clf.score(x_test, y_test) for clf in clfs]

fig, ax = plt.subplots()
ax.set_xlabel("alpha")
ax.set_ylabel("accuracy")
ax.set_title("Accuracy vs alpha for training and testing sets")
ax.plot(ccp_alphas, train_scores, marker='o', label="train",
        drawstyle="steps-post")
ax.plot(ccp_alphas, test_scores, marker='o', label="test",
        drawstyle="steps-post")
ax.legend()
plt.show()